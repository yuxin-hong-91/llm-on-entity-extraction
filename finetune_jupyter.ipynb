{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de3b757a-c05d-4743-a82e-66e239e2328a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Alpaca-LoRA model with params:\n",
      "base_model: /root/.cache/modelscope/hub/modelscope/Llama-2-7b-ms\n",
      "data_path: train_v3.json\n",
      "output_dir: ./lora-alpaca_origin_alpaca_data_gpt4.json\n",
      "batch_size: 1\n",
      "micro_batch_size: 1\n",
      "num_epochs: 5\n",
      "learning_rate: 0.0003\n",
      "cutoff_len: 256\n",
      "val_set_size: 10\n",
      "lora_r: 32\n",
      "lora_alpha: 16\n",
      "lora_dropout: 0.05\n",
      "lora_target_modules: ['q_proj', 'v_proj']\n",
      "train_on_inputs: True\n",
      "add_eos_token: False\n",
      "group_by_length: False\n",
      "wandb_project: \n",
      "wandb_run_name: \n",
      "wandb_watch: \n",
      "wandb_log_model: \n",
      "resume_from_checkpoint: False\n",
      "prompt template: alpaca\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100% 2/2 [00:02<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 16,777,216 || all params: 6,755,192,832 || trainable%: 0.24836028248556738\n",
      "before:\n",
      "-------\n",
      "['instruction', 'input', 'output', 'input_ids', 'attention_mask', 'labels', 'pos', 'neg']\n",
      "train_data\n",
      "{'instruction': 'Please extract the entity of quantity in the input sentence given below , the entity of quantity refers to the entity that represents a specific quantity or unit of measurement in the input sentence .', 'input': 'As a result , a population of just over 100,000 in 1950 has given way to a 2014 estimate of 1,537,058 ( with the metro area estimated at 4,489,109 ) .', 'output': '<im_start> I can extract entities for you, the extracted entities are <<< 4,489,109 >>>  <im_end>', 'input_ids': [1, 13866, 338, 385, 15278, 393, 16612, 263, 3414, 29892, 3300, 2859, 411, 385, 1881, 393, 8128, 4340, 3030, 29889, 14350, 263, 2933, 393, 7128, 2486, 1614, 2167, 278, 2009, 29889, 13, 13, 2277, 29937, 2799, 4080, 29901, 13, 12148, 6597, 278, 7855, 310, 14728, 297, 278, 1881, 10541, 2183, 2400, 1919, 278, 7855, 310, 14728, 14637, 304, 278, 7855, 393, 11524, 263, 2702, 14728, 470, 5190, 310, 20039, 297, 278, 1881, 10541, 869, 13, 13, 2277, 29937, 10567, 29901, 13, 2887, 263, 1121, 1919, 263, 4665, 310, 925, 975, 29871, 29896, 29900, 29900, 29892, 29900, 29900, 29900, 297, 29871, 29896, 29929, 29945, 29900, 756, 2183, 982, 304, 263, 29871, 29906, 29900, 29896, 29946, 12678, 310, 29871, 29896, 29892, 29945, 29941, 29955, 29892, 29900, 29945, 29947, 313, 411, 278, 1539, 307, 4038, 15899, 472, 29871, 29946, 29892, 29946, 29947, 29929, 29892, 29896, 29900, 29929, 1723, 869, 13, 13, 2277, 29937, 13291, 29901, 13, 29966, 326, 29918, 2962, 29958, 306, 508, 6597, 16212, 363, 366, 29892, 278, 23892, 16212, 526, 3532, 29966, 29871, 29946, 29892, 29946, 29947, 29929, 29892, 29896, 29900, 29929, 8653, 29871, 529, 326, 29918, 355, 29958, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [1, 13866, 338, 385, 15278, 393, 16612, 263, 3414, 29892, 3300, 2859, 411, 385, 1881, 393, 8128, 4340, 3030, 29889, 14350, 263, 2933, 393, 7128, 2486, 1614, 2167, 278, 2009, 29889, 13, 13, 2277, 29937, 2799, 4080, 29901, 13, 12148, 6597, 278, 7855, 310, 14728, 297, 278, 1881, 10541, 2183, 2400, 1919, 278, 7855, 310, 14728, 14637, 304, 278, 7855, 393, 11524, 263, 2702, 14728, 470, 5190, 310, 20039, 297, 278, 1881, 10541, 869, 13, 13, 2277, 29937, 10567, 29901, 13, 2887, 263, 1121, 1919, 263, 4665, 310, 925, 975, 29871, 29896, 29900, 29900, 29892, 29900, 29900, 29900, 297, 29871, 29896, 29929, 29945, 29900, 756, 2183, 982, 304, 263, 29871, 29906, 29900, 29896, 29946, 12678, 310, 29871, 29896, 29892, 29945, 29941, 29955, 29892, 29900, 29945, 29947, 313, 411, 278, 1539, 307, 4038, 15899, 472, 29871, 29946, 29892, 29946, 29947, 29929, 29892, 29896, 29900, 29929, 1723, 869, 13, 13, 2277, 29937, 13291, 29901, 13, 29966, 326, 29918, 2962, 29958, 306, 508, 6597, 16212, 363, 366, 29892, 278, 23892, 16212, 526, 3532, 29966, 29871, 29946, 29892, 29946, 29947, 29929, 29892, 29896, 29900, 29929, 8653, 29871, 529, 326, 29918, 355, 29958, 2], 'pos': [1111, 1111], 'neg': [0, 0]}\n",
      "{'instruction': 'Please extract the entity of person in the input sentence given below , the entity of person refers to the entity that represents the identity or role of a specific person in the input sentense .', 'input': 'At the moment Shalev is in Levico Terme , Trento , Italy for a conference .', 'output': '<im_start> I can extract entities for you, the extracted entities are <<< Shalev >>>  <im_end>', 'input_ids': [1, 13866, 338, 385, 15278, 393, 16612, 263, 3414, 29892, 3300, 2859, 411, 385, 1881, 393, 8128, 4340, 3030, 29889, 14350, 263, 2933, 393, 7128, 2486, 1614, 2167, 278, 2009, 29889, 13, 13, 2277, 29937, 2799, 4080, 29901, 13, 12148, 6597, 278, 7855, 310, 2022, 297, 278, 1881, 10541, 2183, 2400, 1919, 278, 7855, 310, 2022, 14637, 304, 278, 7855, 393, 11524, 278, 10110, 470, 6297, 310, 263, 2702, 2022, 297, 278, 1881, 2665, 1947, 869, 13, 13, 2277, 29937, 10567, 29901, 13, 4178, 278, 3256, 1383, 744, 29894, 338, 297, 20708, 1417, 323, 10877, 1919, 1605, 9239, 1919, 12730, 363, 263, 21362, 869, 13, 13, 2277, 29937, 13291, 29901, 13, 29966, 326, 29918, 2962, 29958, 306, 508, 6597, 16212, 363, 366, 29892, 278, 23892, 16212, 526, 3532, 29966, 1383, 744, 29894, 8653, 29871, 529, 326, 29918, 355, 29958, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [1, 13866, 338, 385, 15278, 393, 16612, 263, 3414, 29892, 3300, 2859, 411, 385, 1881, 393, 8128, 4340, 3030, 29889, 14350, 263, 2933, 393, 7128, 2486, 1614, 2167, 278, 2009, 29889, 13, 13, 2277, 29937, 2799, 4080, 29901, 13, 12148, 6597, 278, 7855, 310, 2022, 297, 278, 1881, 10541, 2183, 2400, 1919, 278, 7855, 310, 2022, 14637, 304, 278, 7855, 393, 11524, 278, 10110, 470, 6297, 310, 263, 2702, 2022, 297, 278, 1881, 2665, 1947, 869, 13, 13, 2277, 29937, 10567, 29901, 13, 4178, 278, 3256, 1383, 744, 29894, 338, 297, 20708, 1417, 323, 10877, 1919, 1605, 9239, 1919, 12730, 363, 263, 21362, 869, 13, 13, 2277, 29937, 13291, 29901, 13, 29966, 326, 29918, 2962, 29958, 306, 508, 6597, 16212, 363, 366, 29892, 278, 23892, 16212, 526, 3532, 29966, 1383, 744, 29894, 8653, 29871, 529, 326, 29918, 355, 29958, 2], 'pos': [1111, 1111], 'neg': [0, 0]}\n",
      "cl_loss: tensor(1.1246, device='cuda:0', grad_fn=<NegBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='225' max='225' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [225/225 00:24, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.266800</td>\n",
       "      <td>0.505803</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cl_loss: tensor(1.1119, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(1.2177, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(1.3983, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(1.0539, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(1.0394, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(1.5328, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(1.2355, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(1.2052, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.9058, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(1.2803, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(1.0808, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(1.2050, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(1.3248, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(1.1873, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(1.0333, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(1.0701, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.9031, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(1.0139, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.9980, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.9850, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(1.2740, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.9351, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.9369, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.9513, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.6843, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(1.0418, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.9976, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(1.1031, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.8829, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.8652, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.9948, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.8634, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.9712, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(1.0174, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.7582, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.9029, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(1.1343, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(1.1444, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.9846, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.9909, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(1.0971, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(1.0662, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(1.0268, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(1.2093, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.8346, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.8694, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.8464, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.9664, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.9553, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.9679, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.7540, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.8761, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.9746, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.9314, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(1.0209, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.8655, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.9021, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.9423, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(1.2506, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.8750, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.8702, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.7536, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(1.0982, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.9561, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.9280, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.9357, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(1.0013, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.9396, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.7775, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.8205, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.9822, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.8041, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.7302, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.7807, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.8046, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(1.0654, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.8067, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.8651, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.8853, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.8022, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.9257, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.9463, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.9032, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.9830, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(1.0198, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.7967, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.8490, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.9572, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(1.0350, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.9058, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.9631, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.9213, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.8530, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.9270, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.8956, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(1.2146, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.9954, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.8789, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.9924, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.8561, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(1.0381, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.9451, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.8768, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.8234, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.9715, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.7976, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.7972, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.8780, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(1.0544, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(1.0592, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.8479, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.8819, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(1.0298, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.9172, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.9852, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.8668, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.9174, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.7417, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.9317, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.9490, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.9445, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.9722, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.9956, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.8102, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.7948, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.8093, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.8672, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(1.1196, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.8352, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.7842, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.9549, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.7352, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.7891, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.9499, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(1.1027, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.7946, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.9287, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.8238, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(1.0153, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.9784, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.7258, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.9779, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.8676, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.7458, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.9677, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.9487, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(1.0541, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.8116, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.9765, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.7789, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.9991, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(1.1119, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.7732, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.9464, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.8052, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.9902, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.8610, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.7895, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.8029, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.8892, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.8361, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.8144, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.8758, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.8975, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.8207, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.7902, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.9646, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.8557, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(1.0089, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(1.0393, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.9761, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.8855, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.9048, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.9939, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.8866, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.8149, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(1.2550, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.9035, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.9370, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.8161, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.8790, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.9992, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.8204, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(1.0030, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.8126, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(1.2834, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.8348, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(1.0247, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.8316, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.9400, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.8915, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(1.0337, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(1.1760, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.8730, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(1.0198, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.7865, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.9508, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(1.0245, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(1.1573, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.9599, device='cuda:0')\n",
      "cl_loss: tensor(1.0589, device='cuda:0')\n",
      "cl_loss: tensor(0.9087, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.8013, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(1.0393, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.8568, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.9141, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.8458, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.8880, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(1.0633, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.8449, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.9009, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.8209, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.7953, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.9736, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.9508, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.9264, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(1.0307, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.8391, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.9994, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.7499, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(1.0624, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.9929, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.9983, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.8847, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.9023, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "cl_loss: tensor(0.8389, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "\n",
      " If there's a warning about missing keys above, please disregard :)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from typing import List\n",
    "\n",
    "import fire\n",
    "import torch\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "import copy\n",
    "import math\n",
    "\n",
    "\"\"\"\n",
    "Unused imports:\n",
    "import torch.nn as nn\n",
    "import bitsandbytes as bnb\n",
    "\"\"\"\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    prepare_model_for_int8_training,\n",
    "    set_peft_model_state_dict,\n",
    ")\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "\n",
    "from transformers import Trainer\n",
    "\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "from utils.prompter import Prompter\n",
    "\n",
    "origin = [12148,  6597,   278,  7855,   310]\n",
    "origin = [str(item) for item in origin]\n",
    "\n",
    "prefix = [3532, 29966]\n",
    "prefix = [str(item) for item in prefix]\n",
    "\n",
    "suffix = [8653]\n",
    "suffix = [str(item) for item in suffix]\n",
    "\n",
    "input = [29937, 10567, 29901]\n",
    "input = [str(item) for item in input]\n",
    "\n",
    "\n",
    "def find_last_index(A, B):\n",
    "  B = B[:]\n",
    "  for i, x in reversed(list(enumerate(A))):\n",
    "    if x == B[-1]:\n",
    "      B.pop()\n",
    "      if not B:\n",
    "        return i\n",
    "  return -1\n",
    " \n",
    "\n",
    "\n",
    "def find_first_index(A, B):\n",
    "  B = B[:]\n",
    "  for i, x in enumerate(A):\n",
    "    if x == B[0]:\n",
    "      B.pop(0)\n",
    "      if not B:\n",
    "        return i\n",
    "  return -1\n",
    "\n",
    " \n",
    " \n",
    "def longest_common_sublist(A, B):\n",
    "  matrix = [[0 for _ in range(len(B))] for _ in range(len(A))]\n",
    "  max_len = 0\n",
    "  start_A = -1\n",
    "  end_A = -1\n",
    "  start_B = -1\n",
    "  end_B = -1\n",
    "  for i in range(len(A)):\n",
    "    for j in range(len(B)):\n",
    "      if A[i] == B[j]:\n",
    "        if i == 0 or j == 0:\n",
    "          matrix[i][j] = 1\n",
    "        else:\n",
    "          matrix[i][j] = matrix[i-1][j-1] + 1\n",
    "        if matrix[i][j] >= max_len:\n",
    "          max_len = matrix[i][j]\n",
    "          start_A = i - max_len + 1\n",
    "          end_A = i\n",
    "          start_B = j - max_len + 1\n",
    "          end_B = j\n",
    "      else:\n",
    "        matrix[i][j] = 0\n",
    "  return [start_A, end_A, start_B, end_B]\n",
    "\n",
    "# Create a custom dataset class that inherits from Dataset\n",
    "class CustomCTSDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        super().__init__(dataset.data)\n",
    "        print(\"-------\")\n",
    "        print(self.column_names)\n",
    "\n",
    "class CustomDataCollatorForSeq2Seq(DataCollatorForSeq2Seq):\n",
    "    def __call__(self, features, return_tensors=None):\n",
    "        feat = super().__call__(features, return_tensors=None)\n",
    "#        print(\"feat:\")\n",
    "#        print(feat)\n",
    "        input_ids_batch = feat[\"input_ids\"].tolist()\n",
    "        origin_arr = []\n",
    "        pos_arr = []\n",
    "        neg_arr = []\n",
    "        for input_ids in input_ids_batch:\n",
    "            input_str = \"_\".join([str(item) for item in input_ids])\n",
    "            origin_str = \"_\".join(origin)\n",
    "            prefix_str = \"_\".join(prefix)\n",
    "            suffix_str = \"_\".join(suffix)\n",
    "            sentence_str = \"_\".join(input)\n",
    "\n",
    "            origin_start = input_str.find(origin_str)\n",
    "\n",
    "            origin_index = input_str[0:origin_start + len(origin_str)].count(\"_\") + 1\n",
    "\n",
    "            origin_arr.append(origin_index)\n",
    "\n",
    "            prefix_start = input_str.find(prefix_str)\n",
    "            prefix_index = input_str[0:prefix_start + len(prefix_str)].count(\"_\") + 1\n",
    "\n",
    "            input_start = input_str.find(sentence_str)\n",
    "            input_index = input_str[0:input_start + len(sentence_str)].count(\"_\") + 1\n",
    "\n",
    "\n",
    "            suffix_start = input_str.find(suffix_str)\n",
    "            suffix_index = input_str[0:suffix_start].count(\"_\") - 1\n",
    "\n",
    "            entity_ids = input_ids[prefix_index: suffix_index + 1]\n",
    "            overlap_ids = longest_common_sublist(input_ids[input_index:prefix_index], entity_ids)\n",
    "\n",
    "            sentence_list = input_ids[input_index:prefix_index]\n",
    "            \n",
    "            pos_start = input_index + overlap_ids[0] - overlap_ids[2]\n",
    "            pos_end = input_index + overlap_ids[0] - overlap_ids[2] + len(entity_ids)\n",
    "            pos_arr.append([pos_start, pos_end - 1])\n",
    "            neg_arr.append([pos_start - 1, pos_start - 2, pos_end, pos_end + 1])\n",
    "            \n",
    "        feat[\"origin\"] = torch.tensor(origin_arr)\n",
    "        feat[\"pos\"] = torch.tensor(pos_arr)\n",
    "        feat[\"neg\"] = torch.tensor(neg_arr)\n",
    "\n",
    "        \n",
    "        return feat\n",
    "\n",
    "class ContrastiveTrainer(Trainer):\n",
    "    def contrastive_loss(self, hidden_states, origin, pos, neg):\n",
    "        # hidden_states:  B * L * D\n",
    "        # origin:  B\n",
    "        # pos : B * P\n",
    "        # neg : B * N\n",
    "        # B * 1 * D      pos_e:  B * P * D  neg_e: B * N * D\n",
    "        # P = 2  N = 4\n",
    "\n",
    "        origin_e = torch.gather(hidden_states, 1, origin.unsqueeze(-1).unsqueeze(-1).expand(-1,-1,hidden_states.shape[-1]))\n",
    "        pos_e = torch.gather(hidden_states, 1, pos.unsqueeze(-1).expand(-1,-1,hidden_states.shape[-1]))\n",
    "        neg_e = torch.gather(hidden_states, 1, neg.unsqueeze(-1).expand(-1,-1,hidden_states.shape[-1]))\n",
    "\n",
    "        origin_e = origin_e / origin_e.norm(dim=2, keepdim=True)\n",
    "        pos_e = pos_e / pos_e.norm(dim=2, keepdim=True)\n",
    "        neg_e = neg_e / neg_e.norm(dim=2, keepdim=True)\n",
    "\n",
    "\n",
    "        pos_score = (torch.mul(origin_e, pos_e).sum(dim=2)).sum(dim=1)\n",
    "        neg_score = (torch.mul(origin_e, neg_e).sum(dim=2)).sum(dim=1)\n",
    "\n",
    "        cl_loss = -torch.log(1e-10 + torch.sigmoid(pos_score - neg_score)).mean()\n",
    "\n",
    "        print(\"cl_loss:\", cl_loss)\n",
    "        return cl_loss\n",
    "\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "\n",
    "        origin = copy.deepcopy(inputs[\"origin\"])\n",
    "        pos = copy.deepcopy(inputs[\"pos\"])\n",
    "        neg = copy.deepcopy(inputs[\"neg\"])\n",
    "\n",
    "        if \"origin\" in inputs:\n",
    "            inputs.pop(\"origin\")\n",
    "        if \"pos\" in inputs:\n",
    "            inputs.pop(\"pos\")\n",
    "        if \"neg\" in inputs:\n",
    "            inputs.pop(\"neg\")\n",
    "        \n",
    "        if self.label_smoother is not None and \"labels\" in inputs:\n",
    "            labels = inputs.pop(\"labels\")\n",
    "        else:\n",
    "            labels = None\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "\n",
    "        cts_loss = self.contrastive_loss(outputs[\"hidden_states\"][26], origin, pos, neg)\n",
    "        # Save past state if it exists\n",
    "        # TODO: this needs to be fixed and made cleaner later.\n",
    "        if self.args.past_index >= 0:\n",
    "            self._past = outputs[self.args.past_index]\n",
    "\n",
    "        if labels is not None:\n",
    "            if is_peft_available() and isinstance(model, PeftModel):\n",
    "                model_name = unwrap_model(model.base_model)._get_name()\n",
    "            else:\n",
    "                model_name = unwrap_model(model)._get_name()\n",
    "            if model_name in MODEL_FOR_CAUSAL_LM_MAPPING_NAMES.values():\n",
    "                loss = self.label_smoother(outputs, labels, shift_labels=True)\n",
    "            else:\n",
    "                loss = self.label_smoother(outputs, labels)\n",
    "        else:\n",
    "            if isinstance(outputs, dict) and \"loss\" not in outputs:\n",
    "                raise ValueError(\n",
    "                    \"The model did not return a loss from the inputs, only the following keys: \"\n",
    "                    f\"{','.join(outputs.keys())}. For reference, the inputs it received are {','.join(inputs.keys())}.\"\n",
    "                )\n",
    "            # We don't use .loss here since the model may return tuples instead of ModelOutput.\n",
    "            loss = outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]\n",
    "        \n",
    "        loss += 0.001 * cts_loss\n",
    "#        print(0.0015)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n",
    "\n",
    "def train(\n",
    "    # model/data params\n",
    "    base_model: str = '/root/.cache/modelscope/hub/modelscope/Llama-2-7b-ms',  # the only required argument\n",
    "    data_path: str = \"train_v3.json\", # train_v3.json alpaca_data_gpt4.json\n",
    "    output_dir: str = \"./lora-alpaca_origin_alpaca_data_gpt4.json\",\n",
    "    # training hyperparams\n",
    "    batch_size: int = 1,\n",
    "    micro_batch_size: int = 1,\n",
    "    num_epochs: int = 5,\n",
    "    learning_rate: float = 3e-4,\n",
    "    cutoff_len: int = 256,\n",
    "    val_set_size: int = 10,\n",
    "    # lora hyperparams\n",
    "    lora_r: int = 32,\n",
    "    lora_alpha: int = 16,\n",
    "    lora_dropout: float = 0.05,\n",
    "    lora_target_modules: List[str] = [\n",
    "        \"q_proj\",\n",
    "        \"v_proj\",\n",
    "    ],\n",
    "    # llm hyperparams\n",
    "    train_on_inputs: bool = True,  # if False, masks out inputs in loss\n",
    "    add_eos_token: bool = False,\n",
    "    group_by_length: bool = False,  # faster, but produces an odd training loss curve\n",
    "    # wandb params\n",
    "    wandb_project: str = \"\",\n",
    "    wandb_run_name: str = \"\",\n",
    "    wandb_watch: str = \"\",  # options: false | gradients | all\n",
    "    wandb_log_model: str = \"\",  # options: false | true\n",
    "    resume_from_checkpoint: str = None,  # either training checkpoint or final adapter\n",
    "    prompt_template_name: str = \"alpaca\",  # The prompt template to use, will default to alpaca.\n",
    "):\n",
    "    if int(os.environ.get(\"LOCAL_RANK\", 0)) == 0:\n",
    "        print(\n",
    "            f\"Training Alpaca-LoRA model with params:\\n\"\n",
    "            f\"base_model: {base_model}\\n\"\n",
    "            f\"data_path: {data_path}\\n\"\n",
    "            f\"output_dir: {output_dir}\\n\"\n",
    "            f\"batch_size: {batch_size}\\n\"\n",
    "            f\"micro_batch_size: {micro_batch_size}\\n\"\n",
    "            f\"num_epochs: {num_epochs}\\n\"\n",
    "            f\"learning_rate: {learning_rate}\\n\"\n",
    "            f\"cutoff_len: {cutoff_len}\\n\"\n",
    "            f\"val_set_size: {val_set_size}\\n\"\n",
    "            f\"lora_r: {lora_r}\\n\"\n",
    "            f\"lora_alpha: {lora_alpha}\\n\"\n",
    "            f\"lora_dropout: {lora_dropout}\\n\"\n",
    "            f\"lora_target_modules: {lora_target_modules}\\n\"\n",
    "            f\"train_on_inputs: {train_on_inputs}\\n\"\n",
    "            f\"add_eos_token: {add_eos_token}\\n\"\n",
    "            f\"group_by_length: {group_by_length}\\n\"\n",
    "            f\"wandb_project: {wandb_project}\\n\"\n",
    "            f\"wandb_run_name: {wandb_run_name}\\n\"\n",
    "            f\"wandb_watch: {wandb_watch}\\n\"\n",
    "            f\"wandb_log_model: {wandb_log_model}\\n\"\n",
    "            f\"resume_from_checkpoint: {resume_from_checkpoint or False}\\n\"\n",
    "            f\"prompt template: {prompt_template_name}\\n\"\n",
    "        )\n",
    "    assert (\n",
    "        base_model\n",
    "    ), \"Please specify a --base_model, e.g. --base_model='huggyllama/llama-7b'\"\n",
    "    gradient_accumulation_steps = batch_size // micro_batch_size\n",
    "\n",
    "    prompter = Prompter(prompt_template_name)\n",
    "\n",
    "    device_map = \"auto\"\n",
    "    world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n",
    "    ddp = world_size != 1\n",
    "    if ddp:\n",
    "        device_map = {\"\": int(os.environ.get(\"LOCAL_RANK\") or 0)}\n",
    "        gradient_accumulation_steps = gradient_accumulation_steps // world_size\n",
    "\n",
    "    # Check if parameter passed or if set within environ\n",
    "    use_wandb = len(wandb_project) > 0 or (\n",
    "        \"WANDB_PROJECT\" in os.environ and len(os.environ[\"WANDB_PROJECT\"]) > 0\n",
    "    )\n",
    "    # Only overwrite environ if wandb param passed\n",
    "    if len(wandb_project) > 0:\n",
    "        os.environ[\"WANDB_PROJECT\"] = wandb_project\n",
    "    if len(wandb_watch) > 0:\n",
    "        os.environ[\"WANDB_WATCH\"] = wandb_watch\n",
    "    if len(wandb_log_model) > 0:\n",
    "        os.environ[\"WANDB_LOG_MODEL\"] = wandb_log_model\n",
    "\n",
    "    model = LlamaForCausalLM.from_pretrained(\n",
    "        base_model,\n",
    "        load_in_8bit=False,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=device_map,\n",
    "    )\n",
    "\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(base_model)\n",
    "\n",
    "\n",
    "\n",
    "    tokenizer.pad_token_id = (\n",
    "        0  # unk. we want this to be different from the eos token\n",
    "    )\n",
    "    tokenizer.padding_side = \"left\"  # Allow batched inference\n",
    "\n",
    "    def tokenize(prompt, add_eos_token=True):\n",
    "        # there's probably a way to do this with the tokenizer settings\n",
    "        # but again, gotta move fast\n",
    "        result = tokenizer(\n",
    "            prompt,\n",
    "            truncation=True,\n",
    "            max_length=cutoff_len,\n",
    "            padding=False,\n",
    "            return_tensors=None,\n",
    "        )\n",
    "        if (\n",
    "            result[\"input_ids\"][-1] != tokenizer.eos_token_id\n",
    "            and len(result[\"input_ids\"]) < cutoff_len\n",
    "            and add_eos_token\n",
    "        ):\n",
    "            result[\"input_ids\"].append(tokenizer.eos_token_id)\n",
    "            result[\"attention_mask\"].append(1)\n",
    "\n",
    "        result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "\n",
    "        return result\n",
    "\n",
    "    def generate_and_tokenize_prompt(data_point):\n",
    "        full_prompt = prompter.generate_prompt(\n",
    "            data_point[\"instruction\"],\n",
    "            data_point[\"input\"],\n",
    "            data_point[\"output\"],\n",
    "        )\n",
    "\n",
    "        tokenized_full_prompt = tokenize(full_prompt)\n",
    "        if not train_on_inputs:\n",
    "            user_prompt = prompter.generate_prompt(\n",
    "                data_point[\"instruction\"], data_point[\"input\"]\n",
    "            )\n",
    "            tokenized_user_prompt = tokenize(\n",
    "                user_prompt, add_eos_token=add_eos_token\n",
    "            )\n",
    "            user_prompt_len = len(tokenized_user_prompt[\"input_ids\"])\n",
    "\n",
    "            if add_eos_token:\n",
    "                user_prompt_len -= 1\n",
    "\n",
    "            tokenized_full_prompt[\"labels\"] = [\n",
    "                -100\n",
    "            ] * user_prompt_len + tokenized_full_prompt[\"labels\"][\n",
    "                user_prompt_len:\n",
    "            ]  # could be sped up, probably\n",
    "        tokenized_full_prompt[\"pos\"] = [1111,1111]\n",
    "        tokenized_full_prompt[\"neg\"] = [0000,0000]\n",
    "            \n",
    "        return tokenized_full_prompt\n",
    "\n",
    "    #model = prepare_model_for_int8_training(model)\n",
    "\n",
    "    config = LoraConfig(\n",
    "        r=lora_r,\n",
    "        lora_alpha=lora_alpha,\n",
    "        target_modules=lora_target_modules,\n",
    "        lora_dropout=lora_dropout,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "    model = get_peft_model(model, config)\n",
    "\n",
    "    if data_path.endswith(\".json\") or data_path.endswith(\".jsonl\"):\n",
    "        data = load_dataset(\"json\", data_files=data_path)\n",
    "    else:\n",
    "        data = load_dataset(data_path)\n",
    "\n",
    "    if resume_from_checkpoint:\n",
    "        # Check the available weights and load them\n",
    "        checkpoint_name = os.path.join(\n",
    "            resume_from_checkpoint, \"pytorch_model.bin\"\n",
    "        )  # Full checkpoint\n",
    "        if not os.path.exists(checkpoint_name):\n",
    "            checkpoint_name = os.path.join(\n",
    "                resume_from_checkpoint, \"adapter_model.bin\"\n",
    "            )  # only LoRA model - LoRA config above has to fit\n",
    "            resume_from_checkpoint = (\n",
    "                False  # So the trainer won't try loading its state\n",
    "            )\n",
    "        # The two files above have a different name depending on how they were saved, but are actually the same.\n",
    "        if os.path.exists(checkpoint_name):\n",
    "            print(f\"Restarting from {checkpoint_name}\")\n",
    "            adapters_weights = torch.load(checkpoint_name)\n",
    "            set_peft_model_state_dict(model, adapters_weights)\n",
    "        else:\n",
    "            print(f\"Checkpoint {checkpoint_name} not found\")\n",
    "\n",
    "    model.print_trainable_parameters()  # Be more transparent about the % of trainable params.\n",
    "\n",
    "    print(\"before:\")\n",
    "\n",
    "    if val_set_size > 0:\n",
    "        train_val = data[\"train\"].train_test_split(\n",
    "            test_size=val_set_size, shuffle=True, seed=42\n",
    "        )\n",
    "        \n",
    "#        print(train_val[\"train\"][0])\n",
    "#        print(train_val[\"train\"][1])\n",
    "        \n",
    "        train_data = (\n",
    "            train_val[\"train\"].shuffle().map(generate_and_tokenize_prompt)\n",
    "        )\n",
    "        val_data = (\n",
    "            train_val[\"test\"].shuffle().map(generate_and_tokenize_prompt)\n",
    "        )\n",
    "    else:\n",
    "        train_data = data[\"train\"].shuffle().map(generate_and_tokenize_prompt)\n",
    "        val_data = None\n",
    "\n",
    "#    print(\"after\")\n",
    "#    print(train_data[0])\n",
    "#    print(train_data[1])\n",
    "    \n",
    "    train_data = CustomCTSDataset(train_data)\n",
    "    print(\"train_data\")\n",
    "    print(train_data[0])\n",
    "    print(train_data[1])\n",
    "    \n",
    "    if not ddp and torch.cuda.device_count() > 1:\n",
    "        # keeps Trainer from trying its own DataParallelism when more than 1 gpu is available\n",
    "        model.is_parallelizable = True\n",
    "        model.model_parallel = True\n",
    "\n",
    "    trainer = ContrastiveTrainer(\n",
    "        model=model,\n",
    "        train_dataset=train_data,\n",
    "        eval_dataset=val_data,\n",
    "        args=transformers.TrainingArguments(\n",
    "            per_device_train_batch_size=micro_batch_size,\n",
    "            gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "            warmup_steps=100,\n",
    "            num_train_epochs=num_epochs,\n",
    "            learning_rate=learning_rate,\n",
    "            fp16=True,\n",
    "            logging_steps=10,\n",
    "            optim=\"adamw_torch\",\n",
    "            evaluation_strategy=\"steps\" if val_set_size > 0 else \"no\",\n",
    "            save_strategy=\"steps\",\n",
    "            eval_steps=200 if val_set_size > 0 else None,\n",
    "            save_steps=200,\n",
    "            output_dir=output_dir,\n",
    "            save_total_limit=3,\n",
    "            load_best_model_at_end=True if val_set_size > 0 else False,\n",
    "            ddp_find_unused_parameters=False if ddp else None,\n",
    "            group_by_length=group_by_length,\n",
    "            report_to=\"wandb\" if use_wandb else None,\n",
    "            run_name=wandb_run_name if use_wandb else None,\n",
    "        ),\n",
    "        data_collator=CustomDataCollatorForSeq2Seq(\n",
    "            tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n",
    "        ),\n",
    "    )\n",
    "    model.config.use_cache = False\n",
    "\n",
    "#    old_state_dict = model.state_dict\n",
    "#    model.state_dict = (\n",
    "#        lambda self, *_, **__: get_peft_model_state_dict(\n",
    "#            self, old_state_dict()\n",
    "#        )\n",
    "#    ).__get__(model, type(model))\n",
    "\n",
    "    if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n",
    "        model = torch.compile(model)\n",
    "\n",
    "    trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
    "\n",
    "    model.save_pretrained(output_dir)\n",
    "\n",
    "    print(\n",
    "        \"\\n If there's a warning about missing keys above, please disregard :)\"\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143158bf-34f5-40a5-bbdf-442df9e026d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cc7397-914b-4522-8da3-e4a5753af1c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
