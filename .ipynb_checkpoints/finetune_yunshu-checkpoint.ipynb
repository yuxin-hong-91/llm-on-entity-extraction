{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de3b757a-c05d-4743-a82e-66e239e2328a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dill'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcopy\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Language\\Anaconda3\\lib\\site-packages\\datasets\\__init__.py:37\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m pyarrow\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m version\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_dataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset, concatenate_datasets\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_reader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ReadInstruction\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuilder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArrowBasedBuilder, BeamBasedBuilder, BuilderConfig, DatasetBuilder, GeneratorBasedBuilder\n",
      "File \u001b[1;32mC:\\Language\\Anaconda3\\lib\\site-packages\\datasets\\arrow_dataset.py:58\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPError\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauto\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m---> 58\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_reader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArrowReader\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_writer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArrowWriter, OptimizedTypedSequence\n",
      "File \u001b[1;32mC:\\Language\\Anaconda3\\lib\\site-packages\\datasets\\config.py:8\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpackaging\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m version\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogging\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_logger\n\u001b[0;32m     11\u001b[0m logger \u001b[38;5;241m=\u001b[39m get_logger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Datasets\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Language\\Anaconda3\\lib\\site-packages\\datasets\\utils\\__init__.py:29\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03m\"\"\"Util import.\"\"\"\u001b[39;00m\n\u001b[0;32m     19\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloadConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloadManager\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVersion\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     27\u001b[0m ]\n\u001b[1;32m---> 29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdownload_manager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DownloadConfig, DownloadManager, DownloadMode\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogging\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m disable_progress_bar, enable_progress_bar, is_progress_bar_enabled\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Version\n",
      "File \u001b[1;32mC:\\Language\\Anaconda3\\lib\\site-packages\\datasets\\utils\\download_manager.py:38\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minfo_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_size_checksum_dict\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogging\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_logger, is_progress_bar_enabled\n\u001b[1;32m---> 38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpy_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NestedDataStructure, map_nested, size_str\n\u001b[0;32m     41\u001b[0m logger \u001b[38;5;241m=\u001b[39m get_logger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mDownloadMode\u001b[39;00m(enum\u001b[38;5;241m.\u001b[39mEnum):\n",
      "File \u001b[1;32mC:\\Language\\Anaconda3\\lib\\site-packages\\datasets\\utils\\py_utils.py:35\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CodeType, FunctionType\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Callable, ClassVar, Dict, Generic, Optional, Tuple, Union\n\u001b[1;32m---> 35\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdill\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauto\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'dill'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from typing import List\n",
    "\n",
    "import fire\n",
    "import torch\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "import copy\n",
    "import math\n",
    "\n",
    "\"\"\"\n",
    "Unused imports:\n",
    "import torch.nn as nn\n",
    "import bitsandbytes as bnb\n",
    "\"\"\"\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    prepare_model_for_int8_training,\n",
    "    set_peft_model_state_dict,\n",
    ")\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "\n",
    "from transformers import Trainer\n",
    "\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "from utils.prompter import Prompter\n",
    "\n",
    "origin = [12148,  6597,   278,  7855,   310]\n",
    "origin = [str(item) for item in origin]\n",
    "\n",
    "prefix = [3532, 29966]\n",
    "prefix = [str(item) for item in prefix]\n",
    "\n",
    "suffix = [8653]\n",
    "suffix = [str(item) for item in suffix]\n",
    "\n",
    "input = [29937, 10567, 29901]\n",
    "input = [str(item) for item in input]\n",
    "\n",
    "\n",
    "def find_last_index(A, B):\n",
    "  B = B[:]\n",
    "  for i, x in reversed(list(enumerate(A))):\n",
    "    if x == B[-1]:\n",
    "      B.pop()\n",
    "      if not B:\n",
    "        return i\n",
    "  return -1\n",
    " \n",
    "\n",
    "\n",
    "def find_first_index(A, B):\n",
    "  B = B[:]\n",
    "  for i, x in enumerate(A):\n",
    "    if x == B[0]:\n",
    "      B.pop(0)\n",
    "      if not B:\n",
    "        return i\n",
    "  return -1\n",
    "\n",
    " \n",
    " \n",
    "def longest_common_sublist(A, B):\n",
    "  matrix = [[0 for _ in range(len(B))] for _ in range(len(A))]\n",
    "  max_len = 0\n",
    "  start_A = -1\n",
    "  end_A = -1\n",
    "  start_B = -1\n",
    "  end_B = -1\n",
    "  for i in range(len(A)):\n",
    "    for j in range(len(B)):\n",
    "      if A[i] == B[j]:\n",
    "        if i == 0 or j == 0:\n",
    "          matrix[i][j] = 1\n",
    "        else:\n",
    "          matrix[i][j] = matrix[i-1][j-1] + 1\n",
    "        if matrix[i][j] >= max_len:\n",
    "          max_len = matrix[i][j]\n",
    "          start_A = i - max_len + 1\n",
    "          end_A = i\n",
    "          start_B = j - max_len + 1\n",
    "          end_B = j\n",
    "      else:\n",
    "        matrix[i][j] = 0\n",
    "  return [start_A, end_A, start_B, end_B]\n",
    "\n",
    "# Create a custom dataset class that inherits from Dataset\n",
    "class CustomCTSDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        super().__init__(dataset.data)\n",
    "        print(\"-------\")\n",
    "        print(self.column_names)\n",
    "\n",
    "class CustomDataCollatorForSeq2Seq(DataCollatorForSeq2Seq):\n",
    "    def __call__(self, features, return_tensors=None):\n",
    "        feat = super().__call__(features, return_tensors=None)\n",
    "#        print(\"feat:\")\n",
    "#        print(feat)\n",
    "        input_ids_batch = feat[\"input_ids\"].tolist()\n",
    "        origin_arr = []\n",
    "        pos_arr = []\n",
    "        neg_arr = []\n",
    "        for input_ids in input_ids_batch:\n",
    "            input_str = \"_\".join([str(item) for item in input_ids])\n",
    "            origin_str = \"_\".join(origin)\n",
    "            prefix_str = \"_\".join(prefix)\n",
    "            suffix_str = \"_\".join(suffix)\n",
    "            sentence_str = \"_\".join(input)\n",
    "\n",
    "            origin_start = input_str.find(origin_str)\n",
    "\n",
    "            origin_index = input_str[0:origin_start + len(origin_str)].count(\"_\") + 1\n",
    "\n",
    "            origin_arr.append(origin_index)\n",
    "\n",
    "            prefix_start = input_str.find(prefix_str)\n",
    "            prefix_index = input_str[0:prefix_start + len(prefix_str)].count(\"_\") + 1\n",
    "\n",
    "            input_start = input_str.find(sentence_str)\n",
    "            input_index = input_str[0:input_start + len(sentence_str)].count(\"_\") + 1\n",
    "\n",
    "\n",
    "            suffix_start = input_str.find(suffix_str)\n",
    "            suffix_index = input_str[0:suffix_start].count(\"_\") - 1\n",
    "\n",
    "            entity_ids = input_ids[prefix_index: suffix_index + 1]\n",
    "            overlap_ids = longest_common_sublist(input_ids[input_index:prefix_index], entity_ids)\n",
    "\n",
    "            sentence_list = input_ids[input_index:prefix_index]\n",
    "            \n",
    "            pos_start = input_index + overlap_ids[0] - overlap_ids[2]\n",
    "            pos_end = input_index + overlap_ids[0] - overlap_ids[2] + len(entity_ids)\n",
    "            pos_arr.append([pos_start, pos_end - 1])\n",
    "            neg_arr.append([pos_start - 1, pos_start - 2, pos_end, pos_end + 1])\n",
    "            \n",
    "        feat[\"origin\"] = torch.tensor(origin_arr)\n",
    "        feat[\"pos\"] = torch.tensor(pos_arr)\n",
    "        feat[\"neg\"] = torch.tensor(neg_arr)\n",
    "\n",
    "        \n",
    "        return feat\n",
    "\n",
    "class ContrastiveTrainer(Trainer):\n",
    "    def contrastive_loss(self, hidden_states, origin, pos, neg):\n",
    "        # hidden_states:  B * L * D\n",
    "        # origin:  B\n",
    "        # pos : B * P\n",
    "        # neg : B * N\n",
    "        # B * 1 * D      pos_e:  B * P * D  neg_e: B * N * D\n",
    "        # P = 2  N = 4\n",
    "\n",
    "        origin_e = torch.gather(hidden_states, 1, origin.unsqueeze(-1).unsqueeze(-1).expand(-1,-1,hidden_states.shape[-1]))\n",
    "        pos_e = torch.gather(hidden_states, 1, pos.unsqueeze(-1).expand(-1,-1,hidden_states.shape[-1]))\n",
    "        neg_e = torch.gather(hidden_states, 1, neg.unsqueeze(-1).expand(-1,-1,hidden_states.shape[-1]))\n",
    "\n",
    "        origin_e = origin_e / origin_e.norm(dim=2, keepdim=True)\n",
    "        pos_e = pos_e / pos_e.norm(dim=2, keepdim=True)\n",
    "        neg_e = neg_e / neg_e.norm(dim=2, keepdim=True)\n",
    "\n",
    "\n",
    "        pos_score = (torch.mul(origin_e, pos_e).sum(dim=2)).sum(dim=1)\n",
    "        neg_score = (torch.mul(origin_e, neg_e).sum(dim=2)).sum(dim=1)\n",
    "\n",
    "        cl_loss = -torch.log(1e-10 + torch.sigmoid(pos_score - neg_score)).mean()\n",
    "\n",
    "        print(\"cl_loss:\", cl_loss)\n",
    "        return cl_loss\n",
    "\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "\n",
    "        origin = copy.deepcopy(inputs[\"origin\"])\n",
    "        pos = copy.deepcopy(inputs[\"pos\"])\n",
    "        neg = copy.deepcopy(inputs[\"neg\"])\n",
    "\n",
    "        if \"origin\" in inputs:\n",
    "            inputs.pop(\"origin\")\n",
    "        if \"pos\" in inputs:\n",
    "            inputs.pop(\"pos\")\n",
    "        if \"neg\" in inputs:\n",
    "            inputs.pop(\"neg\")\n",
    "        \n",
    "        if self.label_smoother is not None and \"labels\" in inputs:\n",
    "            labels = inputs.pop(\"labels\")\n",
    "        else:\n",
    "            labels = None\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "\n",
    "        cts_loss = self.contrastive_loss(outputs[\"hidden_states\"][26], origin, pos, neg)\n",
    "        # Save past state if it exists\n",
    "        # TODO: this needs to be fixed and made cleaner later.\n",
    "        if self.args.past_index >= 0:\n",
    "            self._past = outputs[self.args.past_index]\n",
    "\n",
    "        if labels is not None:\n",
    "            if is_peft_available() and isinstance(model, PeftModel):\n",
    "                model_name = unwrap_model(model.base_model)._get_name()\n",
    "            else:\n",
    "                model_name = unwrap_model(model)._get_name()\n",
    "            if model_name in MODEL_FOR_CAUSAL_LM_MAPPING_NAMES.values():\n",
    "                loss = self.label_smoother(outputs, labels, shift_labels=True)\n",
    "            else:\n",
    "                loss = self.label_smoother(outputs, labels)\n",
    "        else:\n",
    "            if isinstance(outputs, dict) and \"loss\" not in outputs:\n",
    "                raise ValueError(\n",
    "                    \"The model did not return a loss from the inputs, only the following keys: \"\n",
    "                    f\"{','.join(outputs.keys())}. For reference, the inputs it received are {','.join(inputs.keys())}.\"\n",
    "                )\n",
    "            # We don't use .loss here since the model may return tuples instead of ModelOutput.\n",
    "            loss = outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]\n",
    "        \n",
    "        loss += 0.001 * cts_loss\n",
    "#        print(0.0015)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n",
    "\n",
    "def train(\n",
    "    # model/data params\n",
    "    base_model: str = r'C:\\Users\\joyho\\.cache\\modelscope\\hub\\modelscope\\Llama-2-7b-ms',  # the only required argument\n",
    "    data_path: str = \"train_v3.json\", # train_v3.json alpaca_data_gpt4.json\n",
    "    output_dir: str = \"./lora-alpaca_origin_alpaca_data_gpt4.json\",\n",
    "    # training hyperparams\n",
    "    batch_size: int = 1,\n",
    "    micro_batch_size: int = 1,\n",
    "    num_epochs: int = 5,\n",
    "    learning_rate: float = 3e-4,\n",
    "    cutoff_len: int = 256,\n",
    "    val_set_size: int = 10,\n",
    "    # lora hyperparams\n",
    "    lora_r: int = 32,\n",
    "    lora_alpha: int = 16,\n",
    "    lora_dropout: float = 0.05,\n",
    "    lora_target_modules: List[str] = [\n",
    "        \"q_proj\",\n",
    "        \"v_proj\",\n",
    "    ],\n",
    "    # llm hyperparams\n",
    "    train_on_inputs: bool = True,  # if False, masks out inputs in loss\n",
    "    add_eos_token: bool = False,\n",
    "    group_by_length: bool = False,  # faster, but produces an odd training loss curve\n",
    "    # wandb params\n",
    "    wandb_project: str = \"\",\n",
    "    wandb_run_name: str = \"\",\n",
    "    wandb_watch: str = \"\",  # options: false | gradients | all\n",
    "    wandb_log_model: str = \"\",  # options: false | true\n",
    "    resume_from_checkpoint: str = None,  # either training checkpoint or final adapter\n",
    "    prompt_template_name: str = \"alpaca\",  # The prompt template to use, will default to alpaca.\n",
    "):\n",
    "    if int(os.environ.get(\"LOCAL_RANK\", 0)) == 0:\n",
    "        print(\n",
    "            f\"Training Alpaca-LoRA model with params:\\n\"\n",
    "            f\"base_model: {base_model}\\n\"\n",
    "            f\"data_path: {data_path}\\n\"\n",
    "            f\"output_dir: {output_dir}\\n\"\n",
    "            f\"batch_size: {batch_size}\\n\"\n",
    "            f\"micro_batch_size: {micro_batch_size}\\n\"\n",
    "            f\"num_epochs: {num_epochs}\\n\"\n",
    "            f\"learning_rate: {learning_rate}\\n\"\n",
    "            f\"cutoff_len: {cutoff_len}\\n\"\n",
    "            f\"val_set_size: {val_set_size}\\n\"\n",
    "            f\"lora_r: {lora_r}\\n\"\n",
    "            f\"lora_alpha: {lora_alpha}\\n\"\n",
    "            f\"lora_dropout: {lora_dropout}\\n\"\n",
    "            f\"lora_target_modules: {lora_target_modules}\\n\"\n",
    "            f\"train_on_inputs: {train_on_inputs}\\n\"\n",
    "            f\"add_eos_token: {add_eos_token}\\n\"\n",
    "            f\"group_by_length: {group_by_length}\\n\"\n",
    "            f\"wandb_project: {wandb_project}\\n\"\n",
    "            f\"wandb_run_name: {wandb_run_name}\\n\"\n",
    "            f\"wandb_watch: {wandb_watch}\\n\"\n",
    "            f\"wandb_log_model: {wandb_log_model}\\n\"\n",
    "            f\"resume_from_checkpoint: {resume_from_checkpoint or False}\\n\"\n",
    "            f\"prompt template: {prompt_template_name}\\n\"\n",
    "        )\n",
    "    assert (\n",
    "        base_model\n",
    "    ), \"Please specify a --base_model, e.g. --base_model='huggyllama/llama-7b'\"\n",
    "    gradient_accumulation_steps = batch_size // micro_batch_size\n",
    "\n",
    "    prompter = Prompter(prompt_template_name)\n",
    "\n",
    "    device_map = \"auto\"\n",
    "    world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n",
    "    ddp = world_size != 1\n",
    "    if ddp:\n",
    "        device_map = {\"\": int(os.environ.get(\"LOCAL_RANK\") or 0)}\n",
    "        gradient_accumulation_steps = gradient_accumulation_steps // world_size\n",
    "\n",
    "    # Check if parameter passed or if set within environ\n",
    "    use_wandb = len(wandb_project) > 0 or (\n",
    "        \"WANDB_PROJECT\" in os.environ and len(os.environ[\"WANDB_PROJECT\"]) > 0\n",
    "    )\n",
    "    # Only overwrite environ if wandb param passed\n",
    "    if len(wandb_project) > 0:\n",
    "        os.environ[\"WANDB_PROJECT\"] = wandb_project\n",
    "    if len(wandb_watch) > 0:\n",
    "        os.environ[\"WANDB_WATCH\"] = wandb_watch\n",
    "    if len(wandb_log_model) > 0:\n",
    "        os.environ[\"WANDB_LOG_MODEL\"] = wandb_log_model\n",
    "\n",
    "    model = LlamaForCausalLM.from_pretrained(\n",
    "        base_model,\n",
    "        load_in_8bit=False,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=device_map,\n",
    "    )\n",
    "\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(base_model)\n",
    "\n",
    "\n",
    "\n",
    "    tokenizer.pad_token_id = (\n",
    "        0  # unk. we want this to be different from the eos token\n",
    "    )\n",
    "    tokenizer.padding_side = \"left\"  # Allow batched inference\n",
    "\n",
    "    def tokenize(prompt, add_eos_token=True):\n",
    "        # there's probably a way to do this with the tokenizer settings\n",
    "        # but again, gotta move fast\n",
    "        result = tokenizer(\n",
    "            prompt,\n",
    "            truncation=True,\n",
    "            max_length=cutoff_len,\n",
    "            padding=False,\n",
    "            return_tensors=None,\n",
    "        )\n",
    "        if (\n",
    "            result[\"input_ids\"][-1] != tokenizer.eos_token_id\n",
    "            and len(result[\"input_ids\"]) < cutoff_len\n",
    "            and add_eos_token\n",
    "        ):\n",
    "            result[\"input_ids\"].append(tokenizer.eos_token_id)\n",
    "            result[\"attention_mask\"].append(1)\n",
    "\n",
    "        result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "\n",
    "        return result\n",
    "\n",
    "    def generate_and_tokenize_prompt(data_point):\n",
    "        full_prompt = prompter.generate_prompt(\n",
    "            data_point[\"instruction\"],\n",
    "            data_point[\"input\"],\n",
    "            data_point[\"output\"],\n",
    "        )\n",
    "\n",
    "        tokenized_full_prompt = tokenize(full_prompt)\n",
    "        if not train_on_inputs:\n",
    "            user_prompt = prompter.generate_prompt(\n",
    "                data_point[\"instruction\"], data_point[\"input\"]\n",
    "            )\n",
    "            tokenized_user_prompt = tokenize(\n",
    "                user_prompt, add_eos_token=add_eos_token\n",
    "            )\n",
    "            user_prompt_len = len(tokenized_user_prompt[\"input_ids\"])\n",
    "\n",
    "            if add_eos_token:\n",
    "                user_prompt_len -= 1\n",
    "\n",
    "            tokenized_full_prompt[\"labels\"] = [\n",
    "                -100\n",
    "            ] * user_prompt_len + tokenized_full_prompt[\"labels\"][\n",
    "                user_prompt_len:\n",
    "            ]  # could be sped up, probably\n",
    "        tokenized_full_prompt[\"pos\"] = [1111,1111]\n",
    "        tokenized_full_prompt[\"neg\"] = [0000,0000]\n",
    "            \n",
    "        return tokenized_full_prompt\n",
    "\n",
    "    #model = prepare_model_for_int8_training(model)\n",
    "\n",
    "    config = LoraConfig(\n",
    "        r=lora_r,\n",
    "        lora_alpha=lora_alpha,\n",
    "        target_modules=lora_target_modules,\n",
    "        lora_dropout=lora_dropout,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "    model = get_peft_model(model, config)\n",
    "\n",
    "    if data_path.endswith(\".json\") or data_path.endswith(\".jsonl\"):\n",
    "        data = load_dataset(\"json\", data_files=data_path)\n",
    "    else:\n",
    "        data = load_dataset(data_path)\n",
    "\n",
    "    if resume_from_checkpoint:\n",
    "        # Check the available weights and load them\n",
    "        checkpoint_name = os.path.join(\n",
    "            resume_from_checkpoint, \"pytorch_model.bin\"\n",
    "        )  # Full checkpoint\n",
    "        if not os.path.exists(checkpoint_name):\n",
    "            checkpoint_name = os.path.join(\n",
    "                resume_from_checkpoint, \"adapter_model.bin\"\n",
    "            )  # only LoRA model - LoRA config above has to fit\n",
    "            resume_from_checkpoint = (\n",
    "                False  # So the trainer won't try loading its state\n",
    "            )\n",
    "        # The two files above have a different name depending on how they were saved, but are actually the same.\n",
    "        if os.path.exists(checkpoint_name):\n",
    "            print(f\"Restarting from {checkpoint_name}\")\n",
    "            adapters_weights = torch.load(checkpoint_name)\n",
    "            set_peft_model_state_dict(model, adapters_weights)\n",
    "        else:\n",
    "            print(f\"Checkpoint {checkpoint_name} not found\")\n",
    "\n",
    "    model.print_trainable_parameters()  # Be more transparent about the % of trainable params.\n",
    "\n",
    "    print(\"before:\")\n",
    "\n",
    "    if val_set_size > 0:\n",
    "        train_val = data[\"train\"].train_test_split(\n",
    "            test_size=val_set_size, shuffle=True, seed=42\n",
    "        )\n",
    "        \n",
    "#        print(train_val[\"train\"][0])\n",
    "#        print(train_val[\"train\"][1])\n",
    "        \n",
    "        train_data = (\n",
    "            train_val[\"train\"].shuffle().map(generate_and_tokenize_prompt)\n",
    "        )\n",
    "        val_data = (\n",
    "            train_val[\"test\"].shuffle().map(generate_and_tokenize_prompt)\n",
    "        )\n",
    "    else:\n",
    "        train_data = data[\"train\"].shuffle().map(generate_and_tokenize_prompt)\n",
    "        val_data = None\n",
    "\n",
    "#    print(\"after\")\n",
    "#    print(train_data[0])\n",
    "#    print(train_data[1])\n",
    "    \n",
    "    train_data = CustomCTSDataset(train_data)\n",
    "    print(\"train_data\")\n",
    "    print(train_data[0])\n",
    "    print(train_data[1])\n",
    "    \n",
    "    if not ddp and torch.cuda.device_count() > 1:\n",
    "        # keeps Trainer from trying its own DataParallelism when more than 1 gpu is available\n",
    "        model.is_parallelizable = True\n",
    "        model.model_parallel = True\n",
    "\n",
    "    trainer = ContrastiveTrainer(\n",
    "        model=model,\n",
    "        train_dataset=train_data,\n",
    "        eval_dataset=val_data,\n",
    "        args=transformers.TrainingArguments(\n",
    "            per_device_train_batch_size=micro_batch_size,\n",
    "            gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "            warmup_steps=100,\n",
    "            num_train_epochs=num_epochs,\n",
    "            learning_rate=learning_rate,\n",
    "            fp16=True,\n",
    "            logging_steps=10,\n",
    "            optim=\"adamw_torch\",\n",
    "            evaluation_strategy=\"steps\" if val_set_size > 0 else \"no\",\n",
    "            save_strategy=\"steps\",\n",
    "            eval_steps=200 if val_set_size > 0 else None,\n",
    "            save_steps=200,\n",
    "            output_dir=output_dir,\n",
    "            save_total_limit=3,\n",
    "            load_best_model_at_end=True if val_set_size > 0 else False,\n",
    "            ddp_find_unused_parameters=False if ddp else None,\n",
    "            group_by_length=group_by_length,\n",
    "            report_to=\"wandb\" if use_wandb else None,\n",
    "            run_name=wandb_run_name if use_wandb else None,\n",
    "        ),\n",
    "        data_collator=CustomDataCollatorForSeq2Seq(\n",
    "            tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n",
    "        ),\n",
    "    )\n",
    "    model.config.use_cache = False\n",
    "\n",
    "#    old_state_dict = model.state_dict\n",
    "#    model.state_dict = (\n",
    "#        lambda self, *_, **__: get_peft_model_state_dict(\n",
    "#            self, old_state_dict()\n",
    "#        )\n",
    "#    ).__get__(model, type(model))\n",
    "\n",
    "    if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n",
    "        model = torch.compile(model)\n",
    "\n",
    "    trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
    "\n",
    "    model.save_pretrained(output_dir)\n",
    "\n",
    "    print(\n",
    "        \"\\n If there's a warning about missing keys above, please disregard :)\"\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cc7397-914b-4522-8da3-e4a5753af1c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
